import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np
import random
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.decomposition import PCA
from scipy.stats import wasserstein_distance
import hashlib
import torch.nn.functional as F

# Configure device and set seeds for reproducibility
def configure_device():
    if torch.cuda.is_available():
        device = torch.device("cuda")
        torch.cuda.set_device(0)
        cuda_generator = torch.Generator(device='cuda')
        cuda_generator.manual_seed(42)
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
    else:
        device = torch.device("cpu")
        cuda_generator = torch.Generator()

    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    return device, cuda_generator

device, cuda_generator = configure_device()
print(f"Using device: {device}")

# Dataset preparation class (unchanged)
class DatasetPreparation:
    @staticmethod
    def get_cifar10_dataset(num_users=100, non_iid_alpha=0.1):
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])

        trainset = torchvision.datasets.CIFAR10(
            root='./data', train=True, download=True, transform=transform
        )

        class_indices = {}
        for idx, (_, label) in enumerate(trainset):
            if label not in class_indices:
                class_indices[label] = []
            class_indices[label].append(idx)

        class_distribution = np.random.dirichlet(
            [non_iid_alpha] * len(class_indices),
            num_users
        )

        user_datasets = {}
        for i in range(num_users):
            user_indices = []
            for label, indices in class_indices.items():
                num_samples = int(class_distribution[i][label] * len(indices))
                user_label_indices = np.random.choice(
                    indices, num_samples, replace=False
                )
                user_indices.extend(user_label_indices)

            user_subset = torch.utils.data.Subset(trainset, user_indices)
            user_loader = torch.utils.data.DataLoader(
                user_subset,
                batch_size=32,
                shuffle=True,
                generator=cuda_generator
            )
            user_datasets[f'user_{i}'] = user_subset

        return user_datasets

# Updated TrapModel class to use a simple CNN
class TrapModel(nn.Module):
    def __init__(self, num_classes=10, use_trap_weights=False):
        super(TrapModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 8 * 8, 512)
        self.fc2 = nn.Linear(512, num_classes)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)

        if use_trap_weights:
            self._initialize_trap_weights()
        else:
            nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')
            nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')

    def _initialize_trap_weights(self):
        with torch.no_grad():
            trap_matrix = torch.zeros_like(self.conv1.weight)
            for i in range(min(trap_matrix.shape[0], trap_matrix.shape[1])):
                trap_matrix[i, i] = 1.0
            trap_matrix += torch.randn_like(trap_matrix) * 0.01
            self.conv1.weight.data = trap_matrix
            self.conv1.bias.data.zero_()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

# Updated SybilUser class with new strategies (unchanged)
class SybilUser:
    def __init__(self, sybil_id, attack_strategy='zero_gradients', device=torch.device('cuda'), trainset=None):
        self.sybil_id = sybil_id
        self.attack_strategy = attack_strategy
        self.device = device
        self.trainset = trainset
        self.is_sybil = True
        self._create_fake_data()

    def _create_fake_data(self):
        if self.attack_strategy == 'zero_gradients':
            self.fake_images = torch.zeros(32, 3, 32, 32).to(self.device)
            self.fake_labels = torch.zeros(32, dtype=torch.long).to(self.device)
        elif self.attack_strategy == 'constant_gradients':
            self.fake_images = torch.ones(32, 3, 32, 32).to(self.device) * 0.1
            self.fake_labels = torch.ones(32, dtype=torch.long).to(self.device)
        elif self.attack_strategy == 'adversarial_zero':
            self.fake_images = torch.randn(32, 3, 32, 32).to(self.device) * 0.01
            self.fake_labels = torch.randint(0, 10, (32,), dtype=torch.long).to(self.device)
        elif self.attack_strategy == 'label_flipping':
            if self.trainset is None:
                raise ValueError("Trainset is required for label_flipping strategy")
            indices = random.sample(range(len(self.trainset)), 100)
            images = [self.trainset[i][0] for i in indices]
            labels = [(self.trainset[i][1] + 1) % 10 for i in indices]
            self.fake_images = torch.stack(images).to(self.device)
            self.fake_labels = torch.tensor(labels, dtype=torch.long).to(self.device)
        elif self.attack_strategy == 'gradient_scaling':
            self.fake_images = torch.randn(100, 3, 32, 32).to(self.device)
            self.fake_labels = torch.randint(0, 10, (100,), dtype=torch.long).to(self.device)
        else:
            raise ValueError(f"Unknown attack strategy: {self.attack_strategy}")
        self.dataset = torch.utils.data.TensorDataset(self.fake_images, self.fake_labels)

    def compute_update(self, global_model, target_user_in_round=False):
        local_model = TrapModel().to(self.device)
        local_model.load_state_dict(global_model.state_dict())

        if self.attack_strategy == 'zero_gradients':
            zero_update = {name: torch.zeros_like(param) for name, param in local_model.named_parameters()}
            return {'update': zero_update, 'local_accuracy': 1.0, 'num_samples': 32, 'sybil_id': self.sybil_id, 'is_sybil': True}
        elif self.attack_strategy == 'constant_gradients':
            constant_update = {name: torch.ones_like(param) * 1e-6 for name, param in local_model.named_parameters()}
            return {'update': constant_update, 'local_accuracy': 0.95, 'num_samples': 32, 'sybil_id': self.sybil_id, 'is_sybil': True}
        else:
            optimizer = optim.SGD(local_model.parameters(), lr=0.001)
            criterion = nn.CrossEntropyLoss()
            data_loader = torch.utils.data.DataLoader(self.dataset, batch_size=32, shuffle=False)

            local_model.train()
            for batch_x, batch_y in data_loader:
                optimizer.zero_grad()
                outputs = local_model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()

                if self.attack_strategy == 'gradient_scaling':
                    scaling_factor = 1.5
                    for param in local_model.parameters():
                        if param.grad is not None:
                            param.grad *= scaling_factor

                optimizer.step()
                break

            update = {name: param.data - global_model.state_dict()[name] for name, param in local_model.named_parameters()}
            local_accuracy = 0.8 if self.attack_strategy == 'adversarial_zero' else 0.9
            num_samples = len(self.dataset)
            return {'update': update, 'local_accuracy': local_accuracy, 'num_samples': num_samples, 'sybil_id': self.sybil_id, 'is_sybil': True}

# Function to create Sybil users (unchanged)
def create_sophisticated_sybil_users(num_sybils, trainset, attack_strategies=None):
    if attack_strategies is None:
        attack_strategies = ['zero_gradients', 'constant_gradients', 'adversarial_zero', 'label_flipping', 'gradient_scaling']
    sybil_users = []
    for i in range(num_sybils):
        strategy = random.choice(attack_strategies)
        sybil = SybilUser(f'sybil_{i}', strategy, device, trainset)
        sybil_users.append(sybil)
    return sybil_users

# EnhancedPrivacyMechanism class with updated detection methods
class EnhancedPrivacyMechanism:
    def __init__(self, base_noise_scale=0.01, num_users=100, device=torch.device('cuda')):
        self.base_noise_scale = base_noise_scale
        self.num_users = num_users
        self.device = device
        self.global_gradient_history = []
        self.sybil_detection_threshold = 0.5
        self.detection_metrics = {
            'update_distances': [],
            'gradient_norms': [],
            'gradient_patterns': [],
            'consistency_scores': [],
            'behavioral_patterns': []
        }
        self.user_behavior_history = {}

    def adaptive_noise_scale(self, grad_norm):
        base_noise = self.base_noise_scale
        adaptive_sigma = base_noise * (1 + 0.1 * torch.log1p(grad_norm))
        return torch.clamp(adaptive_sigma, min=0.001, max=0.05)

    def gradient_sanitizer(self, gradients):
        grad_norm = torch.norm(gradients)
        max_norm = 1.0
        if grad_norm > max_norm:
            clipped_grads = gradients * (max_norm / grad_norm)
        else:
            clipped_grads = gradients
        if self.base_noise_scale > 0:
            noise = torch.normal(mean=0, std=self.base_noise_scale * 0.1, size=clipped_grads.shape).to(clipped_grads.device)
            sanitized_grads = clipped_grads + noise
        else:
            sanitized_grads = clipped_grads
        return sanitized_grads

    def advanced_sybil_detection(self, user_updates, round_num=0):
        try:
            if not user_updates or len(user_updates) < 2:
                return [], [0] * len(user_updates)

            zero_gradient_indices = self._detect_zero_gradients(user_updates)
            behavioral_anomalies = self._detect_behavioral_anomalies(user_updates, round_num)
            norm_anomalies = self._detect_gradient_norm_anomalies(user_updates)
            consistency_anomalies = self._detect_consistency_anomalies(user_updates)
            clustering_anomalies = self._detect_clustering_anomalies(user_updates)
            small_sample_indices = self._detect_small_num_samples(user_updates)

            all_detection_methods = [
                zero_gradient_indices,
                behavioral_anomalies,
                norm_anomalies,
                consistency_anomalies,
                clustering_anomalies,
                small_sample_indices
            ]

            detection_counts = [0] * len(user_updates)
            for method_indices in all_detection_methods:
                for idx in method_indices:
                    detection_counts[idx] += 1

            vote_threshold = 2
            sybil_indices = [idx for idx, count in enumerate(detection_counts) if count >= vote_threshold]

            return sybil_indices, detection_counts
        except Exception as e:
            print(f"Sybil detection error: {str(e)[:200]}")
            return [], [0] * len(user_updates)

    def _detect_zero_gradients(self, user_updates):
        zero_grad_indices = []
        for idx, update in enumerate(user_updates):
            if not isinstance(update, dict) or 'update' not in update:
                continue
            total_norm = 0
            param_count = 0
            for param_name, param_grad in update['update'].items():
                if torch.is_tensor(param_grad):
                    total_norm += torch.norm(param_grad).item()
                    param_count += 1
            avg_norm = total_norm / max(param_count, 1)
            if avg_norm < 1e-5:
                zero_grad_indices.append(idx)
        return zero_grad_indices

    def _detect_behavioral_anomalies(self, user_updates, round_num):
        behavioral_anomalies = []
        for idx, update in enumerate(user_updates):
            if not isinstance(update, dict):
                continue
            user_id = f"user_{idx}"
            if user_id not in self.user_behavior_history:
                self.user_behavior_history[user_id] = {
                    'accuracy_history': [],
                    'norm_history': [],
                    'participation_count': 0
                }
            local_accuracy = update.get('local_accuracy', 0)
            num_samples = update.get('num_samples', 0)
            self.user_behavior_history[user_id]['accuracy_history'].append(local_accuracy)
            self.user_behavior_history[user_id]['participation_count'] += 1
            if len(self.user_behavior_history[user_id]['accuracy_history']) >= 3:
                recent_accuracies = self.user_behavior_history[user_id]['accuracy_history'][-3:]
                if all(acc >= 0.99 for acc in recent_accuracies):
                    behavioral_anomalies.append(idx)
            if num_samples == 32 and local_accuracy == 1.0:
                behavioral_anomalies.append(idx)
        return behavioral_anomalies

    def _detect_gradient_norm_anomalies(self, user_updates):
        norm_anomalies = []
        gradient_norms = []
        for update in user_updates:
            if not isinstance(update, dict) or 'update' not in update:
                gradient_norms.append(0)
                continue
            total_norm = 0
            for param_name, param_grad in update['update'].items():
                if torch.is_tensor(param_grad):
                    total_norm += torch.norm(param_grad).item() ** 2
            gradient_norms.append(np.sqrt(total_norm))
        if len(gradient_norms) > 1:
            norm_array = np.array(gradient_norms)
            q1, q3 = np.percentile(norm_array, [25, 75])
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            for idx, norm in enumerate(gradient_norms):
                if norm < lower_bound or norm > upper_bound:
                    norm_anomalies.append(idx)
        return norm_anomalies

    def _detect_consistency_anomalies(self, user_updates):
        consistency_anomalies = []
        for i in range(len(user_updates)):
            for j in range(i + 1, len(user_updates)):
                if not (isinstance(user_updates[i], dict) and isinstance(user_updates[j], dict)):
                    continue
                if 'update' not in user_updates[i] or 'update' not in user_updates[j]:
                    continue
                similarity = self._calculate_update_similarity(user_updates[i]['update'], user_updates[j]['update'])
                if similarity > 0.9:  # Lowered from 0.95
                    consistency_anomalies.extend([i, j])
        return list(set(consistency_anomalies))

    def _detect_clustering_anomalies(self, user_updates):
        clustering_anomalies = []
        try:
            features = []
            for update in user_updates:
                if not isinstance(update, dict):
                    features.append([0, 0, 0])
                    continue
                grad_norm = 0
                if 'update' in update:
                    for _, param_grad in update['update'].items():
                        if torch.is_tensor(param_grad):
                            grad_norm += torch.norm(param_grad).item()
                accuracy = update.get('local_accuracy', 0)
                num_samples = update.get('num_samples', 0)
                features.append([grad_norm, accuracy, num_samples])
            features = np.array(features)
            if len(features) >= 3:
                centroids = np.mean(features, axis=0)
                distances = np.linalg.norm(features - centroids, axis=1)
                threshold = np.percentile(distances, 20)
                for idx, distance in enumerate(distances):
                    if distance <= threshold and features[idx][0] < 1e-3:
                        clustering_anomalies.append(idx)
        except Exception:
            pass
        return clustering_anomalies

    def _detect_small_num_samples(self, user_updates):
        small_sample_indices = []
        for idx, update in enumerate(user_updates):
            if 'num_samples' in update and update['num_samples'] < 100:
                small_sample_indices.append(idx)
        return small_sample_indices

    def _calculate_update_similarity(self, update1, update2):
        try:
            vec1, vec2 = [], []
            for key in update1.keys():
                if key in update2 and torch.is_tensor(update1[key]) and torch.is_tensor(update2[key]):
                    vec1.extend(update1[key].flatten().cpu().numpy())
                    vec2.extend(update2[key].flatten().cpu().numpy())
            if len(vec1) == 0 or len(vec2) == 0:
                return 0
            vec1, vec2 = np.array(vec1), np.array(vec2)
            dot_product = np.dot(vec1, vec2)
            norm1, norm2 = np.linalg.norm(vec1), np.linalg.norm(vec2)
            if norm1 == 0 or norm2 == 0:
                return 1 if norm1 == norm2 else 0
            return dot_product / (norm1 * norm2)
        except Exception:
            return 0

# Updated FederatedLearningWithDefense class
class FederatedLearningWithDefense:
    def __init__(self, num_users=100, users_per_round=10, num_rounds=100, sybil_fraction=0.3):
        print(f"Initializing Enhanced Defense Framework...")
        print(f"Configuration: {num_users} users, {users_per_round} users/round, {num_rounds} rounds")
        print(f"Sybil injection rate: {sybil_fraction:.1%}")
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        self.num_users = num_users
        self.users_per_round = users_per_round
        self.num_rounds = num_rounds
        self.sybil_fraction = sybil_fraction
        self.device = device
        self.generator = cuda_generator
        print("Preparing Dataset...")
        self.legitimate_datasets = DatasetPreparation.get_cifar10_dataset(int(num_users * (1 - sybil_fraction)))
        print(f"Dataset prepared with {len(self.legitimate_datasets)} legitimate user datasets")
        num_sybils = int(num_users * sybil_fraction)
        print(f"Creating {num_sybils} sophisticated sybil users...")
        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
        self.trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
        self.sybil_users = create_sophisticated_sybil_users(num_sybils, self.trainset)
        self.privacy_mechanism = EnhancedPrivacyMechanism(base_noise_scale=0.01, num_users=num_users, device=self.device)
        self.testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=100, shuffle=False)

    def train_local_model(self, global_model, user_data, is_sybil=False, sybil_user=None):
        if is_sybil and sybil_user:
            return sybil_user.compute_update(global_model, target_user_in_round=True)
        local_model = TrapModel().to(device)
        local_model.load_state_dict(global_model.state_dict())
        optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)
        criterion = nn.CrossEntropyLoss()
        def custom_collate(batch):
            images = torch.stack([item[0] for item in batch])
            labels = torch.tensor([item[1] for item in batch])
            return images, labels
        data_loader = torch.utils.data.DataLoader(user_data, batch_size=32, shuffle=True, collate_fn=custom_collate, pin_memory=False)
        noise_scale = float(self.privacy_mechanism.adaptive_noise_scale(torch.norm(torch.cat([p.view(-1) for p in local_model.parameters()]))))
        local_model.train()
        local_accuracy = 0
        total_samples = 0
        for epoch in range(10):  # Increased from 5 to 10
            for batch_x, batch_y in data_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                optimizer.zero_grad()
                outputs = local_model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                for param in local_model.parameters():
                    if param.grad is not None:
                        if noise_scale > 0:
                            noise = torch.zeros_like(param.grad).normal_(mean=0, std=noise_scale * 0.1)
                            param.grad += noise
                        torch.nn.utils.clip_grad_norm_([param], max_norm=1.0)
                optimizer.step()
                with torch.no_grad():
                    _, predicted = torch.max(outputs, 1)
                    local_accuracy += (predicted == batch_y).sum().item()
                    total_samples += batch_y.size(0)
        local_accuracy /= total_samples
        local_update = {
            'update': {name: local_model.state_dict()[name] - global_model.state_dict()[name] for name in global_model.state_dict()},
            'local_accuracy': local_accuracy,
            'num_samples': total_samples,
            'is_sybil': False
        }
        return local_update

    def aggregate_updates(self, local_updates, round_num=0):
        sybil_indices, detection_counts = self.privacy_mechanism.advanced_sybil_detection(local_updates, round_num)
        print(f"Round {round_num}: Detected {len(sybil_indices)} potential Sybil users")

        legitimate_updates = [update for idx, update in enumerate(local_updates) if idx not in sybil_indices]

        if not legitimate_updates:
            print("All updates detected as Sybils; skipping aggregation.")
            return {'aggregated_update': {}, 'sybil_detected': len(sybil_indices), 'total_updates': len(local_updates), 'detection_accuracy': self._calculate_detection_accuracy(sybil_indices, local_updates)}

        total_samples = sum(update['num_samples'] for update in legitimate_updates)
        aggregated_update = {}
        for key in legitimate_updates[0]['update'].keys():
            weighted_sum = sum(update['num_samples'] * update['update'][key] for update in legitimate_updates)
            aggregated_update[key] = weighted_sum / total_samples

        return {
            'aggregated_update': aggregated_update,
            'sybil_detected': len(sybil_indices),
            'total_updates': len(local_updates),
            'detection_accuracy': self._calculate_detection_accuracy(sybil_indices, local_updates)
        }

    def _calculate_detection_accuracy(self, detected_indices, all_updates):
        true_positives = 0
        false_positives = 0
        true_negatives = 0
        false_negatives = 0
        for i, update in enumerate(all_updates):
            is_actually_sybil = update.get('is_sybil', False)
            is_detected_as_sybil = i in detected_indices
            if is_actually_sybil and is_detected_as_sybil:
                true_positives += 1
            elif is_actually_sybil and not is_detected_as_sybil:
                false_negatives += 1
            elif not is_actually_sybil and is_detected_as_sybil:
                false_positives += 1
            else:
                true_negatives += 1
        total = len(all_updates)
        accuracy = (true_positives + true_negatives) / total if total > 0 else 0
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'true_positives': true_positives,
            'false_positives': false_positives,
            'true_negatives': true_negatives,
            'false_negatives': false_negatives
        }

    def test_accuracy(self, model):
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in self.test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        return correct / total

    def calculate_reconstruction_snr(self, model):
        try:
            random_inputs = torch.randn(10, 3, 32, 32).to(device)
            activations = []
            def hook_fn(module, input, output):
                activations.append(output.detach())
            hook = model.conv1.register_forward_hook(hook_fn)
            model(random_inputs)
            hook.remove()
            if activations:
                layer_output = activations[0]
                signal_power = torch.mean(layer_output ** 2)
                noise_power = torch.var(layer_output, unbiased=False)
                eps = 1e-8
                snr = 10 * torch.log10(signal_power / (noise_power + eps))
                snr = torch.where(torch.isfinite(snr), snr, torch.tensor(0.0).to(device))
                return torch.clamp(snr, min=-50, max=100).item()
            return 0
        except Exception as e:
            print(f"SNR calculation error: {str(e)[:200]}")
            return 0

    def federated_training(self, use_trap_weights=False):
        global_model = TrapModel(use_trap_weights=use_trap_weights).to(device)
        metrics = {
            'global_accuracy': [],
            'reconstruction_snr': [],
            'sybil_detected': [],
            'local_accuracies': [],
            'detection_accuracy': [],
            'detection_precision': [],
            'detection_recall': [],
            'total_sybil_count': 0,
            'legitimate_users_flagged': [],
            'attack_success_rate': []
        }
        print(f"Starting Enhanced Defense Training with {len(self.sybil_users)} sybil users...")
        print(f"Using trap weights: {use_trap_weights}")
        for round_num in range(self.num_rounds):
            print(f"\nRound {round_num + 1}/{self.num_rounds}")
            legitimate_users = list(self.legitimate_datasets.keys())
            if round_num % 10 == 0 and len(self.sybil_users) >= self.users_per_round - 1:
                target_user = random.choice(legitimate_users)
                selected_sybils = random.sample(self.sybil_users, self.users_per_round - 1)
                selected_users = [(target_user, False)] + [(sybil, True) for sybil in selected_sybils]
                print(f"ATTACK ROUND: 1 target user + {len(selected_sybils)} sybils")
            else:
                num_sybils_in_round = min(random.randint(0, min(3, len(self.sybil_users))), self.users_per_round // 3)
                num_legitimate = self.users_per_round - num_sybils_in_round
                selected_legitimate = random.sample(legitimate_users, num_legitimate)
                selected_sybils = random.sample(self.sybil_users, num_sybils_in_round)
                selected_users = ([(user, False) for user in selected_legitimate] + [(sybil, True) for sybil in selected_sybils])
                print(f"Normal round: {num_legitimate} legitimate + {num_sybils_in_round} sybils")
            local_updates = []
            local_accuracies = []
            for user, is_sybil in selected_users:
                if is_sybil:
                    update = self.train_local_model(global_model, None, is_sybil=True, sybil_user=user)
                else:
                    user_data = self.legitimate_datasets[user]
                    update = self.train_local_model(global_model, user_data)
                local_updates.append(update)
                local_accuracies.append(update['local_accuracy'])
            aggregation_result = self.aggregate_updates(local_updates, round_num)
            if aggregation_result['aggregated_update']:
                global_lr = 1.0
                for name, param in global_model.state_dict().items():
                    if name in aggregation_result['aggregated_update']:
                        update = aggregation_result['aggregated_update'][name] * global_lr
                        param.add_(update)
            detection_stats = aggregation_result.get('detection_accuracy', {})
            metrics['global_accuracy'].append(self.test_accuracy(global_model))
            metrics['reconstruction_snr'].append(self.calculate_reconstruction_snr(global_model))
            metrics['sybil_detected'].append(aggregation_result['sybil_detected'])
            metrics['local_accuracies'].append(np.mean(local_accuracies))
            metrics['detection_accuracy'].append(detection_stats.get('accuracy', 0))
            metrics['detection_precision'].append(detection_stats.get('precision', 0))
            metrics['detection_recall'].append(detection_stats.get('recall', 0))
            metrics['total_sybil_count'] += aggregation_result['sybil_detected']
            metrics['legitimate_users_flagged'].append(detection_stats.get('false_positives', 0))
            actual_sybils = sum(1 for _, is_sybil in selected_users if is_sybil)
            detected_sybils = aggregation_result['sybil_detected']
            attack_success = max(0, actual_sybils - detected_sybils) / max(actual_sybils, 1)
            metrics['attack_success_rate'].append(attack_success)
            print(f"  Global Accuracy: {metrics['global_accuracy'][-1]:.4f}")
            print(f"  Sybils Detected: {detected_sybils}/{actual_sybils}")
            print(f"  Detection Accuracy: {detection_stats.get('accuracy', 0):.4f}")
            print(f"  Attack Success Rate: {attack_success:.4f}")
        print("\nEnhanced Defense Training Completed.")
        return metrics

    def visualize_results(self, metrics):
        plt.figure(figsize=(20, 15))
        plt.subplot(3, 4, 1)
        plt.plot(metrics['global_accuracy'], label='Global Accuracy', color='blue', linewidth=2)
        plt.title('Global Model Accuracy Over Time')
        plt.xlabel('Training Round')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.subplot(3, 4, 2)
        plt.plot(metrics['sybil_detected'], label='Sybils Detected', color='red', linewidth=2)
        plt.title('Sybil Users Detected per Round')
        plt.xlabel('Training Round')
        plt.ylabel('Number of Sybils Detected')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.subplot(3, 4, 3)
        plt.plot(metrics['detection_accuracy'], label='Accuracy', color='green', linewidth=2)
        plt.plot(metrics['detection_precision'], label='Precision', color='orange', linewidth=2)
        plt.plot(metrics['detection_recall'], label='Recall', color='purple', linewidth=2)
        plt.title('Detection Performance Metrics')
        plt.xlabel('Training Round')
        plt.ylabel('Score')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.subplot(3, 4, 4)
        plt.plot(metrics['attack_success_rate'], label='Attack Success Rate', color='darkred', linewidth=2)
        plt.title('Attack Success Rate (Lower is Better)')
        plt.xlabel('Training Round')
        plt.ylabel('Success Rate')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.subplot(3, 4, 5)
        plt.plot(metrics['reconstruction_snr'], label='Reconstruction SNR', color='brown', linewidth=2)
        plt.title('Data Reconstruction Signal-to-Noise Ratio')
        plt.xlabel('Training Round')
        plt.ylabel('SNR (dB)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.subplot(3, 4, 6)
        plt.plot(metrics['local_accuracies'], label='Avg Local Accuracy', color='teal', linewidth=2)
        plt.title('Average Local Model Accuracies')
        plt.xlabel('Training Round')
        plt.ylabel('Local Accuracy')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.subplot(3, 4, 7)
        plt.plot(metrics['legitimate_users_flagged'], label='False Positives', color='orange', linewidth=2)
        plt.title('Legitimate Users Incorrectly Flagged')
        plt.xlabel('Training Round')
        plt.ylabel('Count')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.subplot(3, 4, 8)
        summary_stats = [
            metrics['total_sybil_count'],
            np.mean(metrics['detection_accuracy']) * 100,
            np.mean(metrics['attack_success_rate']) * 100,
            metrics['global_accuracy'][-1] * 100 if metrics['global_accuracy'] else 0
        ]
        labels = ['Total Sybils\nDetected', 'Avg Detection\nAccuracy (%)', 'Avg Attack\nSuccess (%)', 'Final Model\nAccuracy (%)']
        colors = ['red', 'green', 'darkred', 'blue']
        plt.bar(labels, summary_stats, color=colors, alpha=0.7)
        plt.title('Summary Statistics')
        plt.ylabel('Value')
        plt.xticks(rotation=45)
        plt.subplot(3, 4, 9)
        plt.hist(metrics['detection_accuracy'], bins=20, alpha=0.7, color='green', edgecolor='black')
        plt.title('Distribution of Detection Accuracies')
        plt.xlabel('Detection Accuracy')
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
        plt.subplot(3, 4, 10)
        plt.scatter(metrics['detection_accuracy'], metrics['attack_success_rate'], alpha=0.6, color='purple', s=30)
        plt.title('Detection Accuracy vs Attack Success')
        plt.xlabel('Detection Accuracy')
        plt.ylabel('Attack Success Rate')
        plt.grid(True, alpha=0.3)
        plt.subplot(3, 4, 11)
        cumulative_sybils = np.cumsum(metrics['sybil_detected'])
        plt.plot(cumulative_sybils, label='Cumulative Sybils Detected', color='darkred', linewidth=2)
        plt.title('Cumulative Sybil Detection Over Time')
        plt.xlabel('Training Round')
        plt.ylabel('Total Sybils Detected')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.subplot(3, 4, 12)
        attack_rounds = [i for i, rate in enumerate(metrics['attack_success_rate']) if rate > 0.5]
        normal_rounds = [i for i, rate in enumerate(metrics['attack_success_rate']) if rate <= 0.5]
        if attack_rounds and normal_rounds:
            attack_acc = [metrics['global_accuracy'][i] for i in attack_rounds if i < len(metrics['global_accuracy'])]
            normal_acc = [metrics['global_accuracy'][i] for i in normal_rounds if i < len(metrics['global_accuracy'])]
            plt.boxplot([normal_acc, attack_acc], labels=['Normal Rounds', 'Attack Rounds'])
            plt.title('Model Accuracy: Normal vs Attack Rounds')
            plt.ylabel('Accuracy')
        else:
            plt.text(0.5, 0.5, 'Insufficient data for comparison', transform=plt.gca().transAxes, ha='center', va='center')
            plt.title('Model Accuracy: Normal vs Attack Rounds')
        plt.tight_layout()
        plt.show()

# Main function (unchanged)
def main():
    print("="*80)
    print("ENHANCED FEDERATED LEARNING DEFENSE FRAMEWORK")
    print("Defense Against Trap Weight Attacks (Baseline Paper Reproduction)")
    print("="*80)
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
        torch.cuda.manual_seed_all(42)
    config = {'num_users': 100, 'users_per_round': 10, 'num_rounds': 50, 'sybil_fraction': 0.2}
    print(f"Experimental Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print()
    experiment = FederatedLearningWithDefense(**config)
    print("\n" + "="*80)
    print("EXPERIMENT: Defense vs Trap Weight Attack (Baseline Paper)")
    print("="*80)
    print("Running defense against trap weight attack (baseline paper scenario)...")
    trap_metrics = experiment.federated_training(use_trap_weights=True)
    print("\n" + "="*60)
    print("RESULTS VISUALIZATION")
    print("="*60)
    print("Generating visualizations for trap weight attack defense...")
    experiment.visualize_results(trap_metrics)
    print("\n" + "="*80)
    print("COMPREHENSIVE EXPERIMENTAL RESULTS SUMMARY")
    print("="*80)
    def print_experiment_summary(metrics, experiment_name):
        print(f"\n{experiment_name.upper()} RESULTS:")
        print("-" * 50)
        if metrics['global_accuracy']:
            print(f"Final Global Accuracy: {metrics['global_accuracy'][-1]:.4f}")
            print(f"Average Global Accuracy: {np.mean(metrics['global_accuracy']):.4f}")
        if metrics['detection_accuracy']:
            print(f"Average Detection Accuracy: {np.mean(metrics['detection_accuracy']):.4f}")
            print(f"Best Detection Accuracy: {max(metrics['detection_accuracy']):.4f}")
        if metrics['detection_precision']:
            print(f"Average Detection Precision: {np.mean(metrics['detection_precision']):.4f}")
        if metrics['detection_recall']:
            print(f"Average Detection Recall: {np.mean(metrics['detection_recall']):.4f}")
        print(f"Total Sybils Detected: {metrics['total_sybil_count']}")
        if metrics['attack_success_rate']:
            print(f"Average Attack Success Rate: {np.mean(metrics['attack_success_rate']):.4f}")
            print(f"Defense Success Rate: {1 - np.mean(metrics['attack_success_rate']):.4f}")
        if metrics['legitimate_users_flagged']:
            print(f"Average False Positives: {np.mean(metrics['legitimate_users_flagged']):.2f}")
            print(f"Total Legitimate Users Flagged: {sum(metrics['legitimate_users_flagged'])}")
        if metrics['reconstruction_snr']:
            print(f"Average Reconstruction SNR: {np.mean(metrics['reconstruction_snr']):.2f} dB")
    print_experiment_summary(trap_metrics, "Trap Weight Attack Defense (Baseline Paper)")
    print(f"\n{'FINAL ASSESSMENT':^80}")
    print("=" * 80)
    if trap_metrics['detection_accuracy'] and np.mean(trap_metrics['detection_accuracy']) > 0.6:
        print("🎯 SUCCESS: Our enhanced defense framework successfully detects")
        print("   sophisticated trap weight attacks described in the baseline paper!")
        print(f"   Average detection accuracy: {np.mean(trap_metrics['detection_accuracy']):.1%}")
    else:
        print("❌ CHALLENGE: Defense needs further improvement to effectively")
        print("   counter the sophisticated attacks from the baseline paper.")
    print(f"\n📈 Framework Performance:")
    if trap_metrics['detection_precision']:
        avg_precision = np.mean(trap_metrics['detection_precision'])
        avg_recall = np.mean(trap_metrics['detection_recall'])
        print(f"   • Precision: {avg_precision:.1%}")
        print(f"   • Recall: {avg_recall:.1%}")
    if trap_metrics['legitimate_users_flagged']:
        avg_false_positives = np.mean(trap_metrics['legitimate_users_flagged'])
        print(f"   • Average False Positives per Round: {avg_false_positives:.1f}")
    print(f"\n🔍 Detailed Attack Analysis:")
    if trap_metrics['attack_success_rate']:
        attack_success_rates = trap_metrics['attack_success_rate']
        successful_attacks = sum(1 for rate in attack_success_rates if rate > 0.5)
        total_rounds = len(attack_success_rates)
        print(f"   • Rounds with successful attacks: {successful_attacks}/{total_rounds}")
        print(f"   • Defense effectiveness: {((total_rounds - successful_attacks) / total_rounds * 100):.1f}%")
    if trap_metrics['reconstruction_snr']:
        avg_snr = np.mean(trap_metrics['reconstruction_snr'])
        if avg_snr < 10:
            print(f"   • Low reconstruction SNR ({avg_snr:.1f} dB) indicates good privacy protection")
        else:
            print(f"   • High reconstruction SNR ({avg_snr:.1f} dB) indicates potential privacy risk")
    print("\n" + "="*80)
    print("EXPERIMENT COMPLETED SUCCESSFULLY!")
    print("Defense framework evaluation against baseline paper trap weight attacks finished.")
    print("="*80)

if __name__ == "__main__":
    main()